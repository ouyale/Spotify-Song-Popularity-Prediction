{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 5: Alternative Models\n",
    "\n",
    "## Spotify Song Popularity Prediction\n",
    "\n",
    "**Goal:** Explore models beyond linear regression and basic tree ensembles to improve our best CV RMSE of 10.42.\n",
    "\n",
    "### Models to Test:\n",
    "1. **SVR (Support Vector Regression)** - RBF and Linear kernels\n",
    "2. **KNN Regression** - Distance-based predictions\n",
    "3. **XGBoost** - Advanced gradient boosting\n",
    "4. **LightGBM** - Fast gradient boosting\n",
    "5. **Bayesian Ridge** - Automatic regularization\n",
    "\n",
    "### Pipeline:\n",
    "- Same leakage-safe methodology as Approach 3 v2\n",
    "- 5-fold CV with preprocessing inside each fold\n",
    "- Hyperparameter tuning for top performers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Previous best models (for comparison)\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# NEW models to test\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# XGBoost and LightGBM\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "    print(\"XGBoost version:\", xgb.__version__)\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"XGBoost not installed. Run: pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "    print(\"LightGBM version:\", lgb.__version__)\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "    print(\"LightGBM not installed. Run: pip install lightgbm\")\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"\\nAll libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('/Users/barbarawerobaobayi/Documents/Strathclyde/Semester 2/Machine Learning for Data Analytics/Spotify Project/data/CS98XRegressionTrain.csv')\n",
    "test_df = pd.read_csv('/Users/barbarawerobaobayi/Documents/Strathclyde/Semester 2/Machine Learning for Data Analytics/Spotify Project/data/CS98XRegressionTest.csv')\n",
    "\n",
    "# Handle missing genres\n",
    "train_df['top genre'] = train_df['top genre'].fillna('Unknown').replace('', 'Unknown')\n",
    "test_df['top genre'] = test_df['top genre'].fillna('Unknown').replace('', 'Unknown')\n",
    "\n",
    "print(f\"Training set: {train_df.shape[0]} rows, {train_df.shape[1]} columns\")\n",
    "print(f\"Test set:     {test_df.shape[0]} rows, {test_df.shape[1]} columns\")\n",
    "print(f\"\\nTarget variable (pop) statistics:\")\n",
    "print(train_df['pop'].describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Feature Engineering Functions\n",
    "\n",
    "Reusing the same pipeline from Approach 3 v2 (leakage-safe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "TOP_N_GENRES = 15\n",
    "numerical_features = ['bpm', 'nrgy', 'dnce', 'dB', 'live', 'val', 'dur', 'acous', 'spch']\n",
    "\n",
    "def encode_genres(df, top_genres):\n",
    "    \"\"\"\n",
    "    Encode genres using pre-defined top genres list.\n",
    "    Genres not in the list are mapped to 'other'.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['genre_simplified'] = df['top genre'].apply(\n",
    "        lambda x: x if x in top_genres else 'other'\n",
    "    )\n",
    "    genre_dummies = pd.get_dummies(df['genre_simplified'], prefix='genre')\n",
    "    df = pd.concat([df, genre_dummies], axis=1)\n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Create engineered features from the original numerical features.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # INTERACTION TERMS\n",
    "    df['nrgy_x_dnce'] = df['nrgy'] * df['dnce']\n",
    "    df['nrgy_x_val'] = df['nrgy'] * df['val']\n",
    "    df['nrgy_x_dB'] = df['nrgy'] * df['dB']\n",
    "    df['dnce_x_val'] = df['dnce'] * df['val']\n",
    "    df['dnce_x_bpm'] = df['dnce'] * df['bpm']\n",
    "    df['acous_x_nrgy'] = df['acous'] * df['nrgy']\n",
    "    \n",
    "    # RATIOS\n",
    "    df['nrgy_per_bpm'] = df['nrgy'] / (df['bpm'] + 1)\n",
    "    df['dnce_per_nrgy'] = df['dnce'] / (df['nrgy'] + 1)\n",
    "    df['val_per_nrgy'] = df['val'] / (df['nrgy'] + 1)\n",
    "    df['spch_per_dur'] = df['spch'] / (df['dur'] + 1)\n",
    "    \n",
    "    # POLYNOMIAL FEATURES\n",
    "    df['dur_squared'] = df['dur'] ** 2\n",
    "    df['acous_squared'] = df['acous'] ** 2\n",
    "    df['dB_squared'] = df['dB'] ** 2\n",
    "    df['nrgy_squared'] = df['nrgy'] ** 2\n",
    "    \n",
    "    # BINNED FEATURES\n",
    "    df['bpm_slow'] = (df['bpm'] < 100).astype(int)\n",
    "    df['bpm_medium'] = ((df['bpm'] >= 100) & (df['bpm'] < 130)).astype(int)\n",
    "    df['bpm_fast'] = (df['bpm'] >= 130).astype(int)\n",
    "    df['low_energy'] = (df['nrgy'] < 50).astype(int)\n",
    "    df['high_energy'] = (df['nrgy'] >= 70).astype(int)\n",
    "    df['is_acoustic'] = (df['acous'] > 50).astype(int)\n",
    "    df['short_song'] = (df['dur'] < 180).astype(int)\n",
    "    df['long_song'] = (df['dur'] > 300).astype(int)\n",
    "    \n",
    "    # COMPOSITE SCORES\n",
    "    df['party_score'] = (df['nrgy'] + df['dnce'] + df['val'] - df['acous']) / 4\n",
    "    df['chill_score'] = (df['acous'] + (100 - df['nrgy']) + (100 - df['dnce'])) / 3\n",
    "    df['vocal_score'] = df['spch'] + df['live']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Engineered features list\n",
    "engineered_features = [\n",
    "    'nrgy_x_dnce', 'nrgy_x_val', 'nrgy_x_dB', 'dnce_x_val', 'dnce_x_bpm', 'acous_x_nrgy',\n",
    "    'nrgy_per_bpm', 'dnce_per_nrgy', 'val_per_nrgy', 'spch_per_dur',\n",
    "    'dur_squared', 'acous_squared', 'dB_squared', 'nrgy_squared',\n",
    "    'bpm_slow', 'bpm_medium', 'bpm_fast', 'low_energy', 'high_energy',\n",
    "    'is_acoustic', 'short_song', 'long_song',\n",
    "    'party_score', 'chill_score', 'vocal_score'\n",
    "]\n",
    "\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print(f\"Engineered features: {len(engineered_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Leakage-Safe Cross-Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pipeline_cv(df, features_to_use, model, scale=True, cv=5, verbose=False):\n",
    "    \"\"\"\n",
    "    Proper cross-validation with encoding done inside each fold.\n",
    "    This prevents ANY data leakage.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    val_rmses = []\n",
    "    val_r2s = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "        # Split THIS fold\n",
    "        fold_train = df.iloc[train_idx].copy()\n",
    "        fold_val = df.iloc[val_idx].copy()\n",
    "        \n",
    "        # Learn top genres from THIS FOLD's training data only\n",
    "        fold_top_genres = fold_train['top genre'].value_counts().head(TOP_N_GENRES).index.tolist()\n",
    "        \n",
    "        # Encode genres\n",
    "        fold_train_enc = encode_genres(fold_train, fold_top_genres)\n",
    "        fold_val_enc = encode_genres(fold_val, fold_top_genres)\n",
    "        \n",
    "        # Align columns\n",
    "        fold_genre_cols = [c for c in fold_train_enc.columns if c.startswith('genre_') and c != 'genre_simplified']\n",
    "        for col in fold_genre_cols:\n",
    "            if col not in fold_val_enc.columns:\n",
    "                fold_val_enc[col] = 0\n",
    "        \n",
    "        # Apply feature engineering\n",
    "        fold_train_fe = engineer_features(fold_train_enc)\n",
    "        fold_val_fe = engineer_features(fold_val_enc)\n",
    "        \n",
    "        # Get available features\n",
    "        available_features = [f for f in features_to_use if f in fold_train_fe.columns]\n",
    "        \n",
    "        X_train = fold_train_fe[available_features].copy()\n",
    "        X_val = fold_val_fe[available_features].copy()\n",
    "        \n",
    "        # Handle missing columns in validation\n",
    "        for col in available_features:\n",
    "            if col not in X_val.columns:\n",
    "                X_val[col] = 0\n",
    "        X_val = X_val[available_features]\n",
    "        \n",
    "        y_train = fold_train_fe['pop']\n",
    "        y_val = fold_val_fe['pop']\n",
    "        \n",
    "        # Scale if needed\n",
    "        if scale:\n",
    "            scaler = StandardScaler()\n",
    "            X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "            X_val = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns, index=X_val.index)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        model.fit(X_train.values, y_train.values)\n",
    "        y_pred = model.predict(X_val.values)\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        \n",
    "        val_rmses.append(rmse)\n",
    "        val_r2s.append(r2)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Fold {fold+1}: RMSE = {rmse:.4f}, R² = {r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'cv_rmse': np.mean(val_rmses),\n",
    "        'cv_rmse_std': np.std(val_rmses),\n",
    "        'cv_r2': np.mean(val_r2s),\n",
    "        'cv_r2_std': np.std(val_r2s),\n",
    "        'fold_rmses': val_rmses\n",
    "    }\n",
    "\n",
    "print(\"Leakage-safe CV function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Prepare Feature List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get genre features from full training data (for reference)\n",
    "top_genres = train_df['top genre'].value_counts().head(TOP_N_GENRES).index.tolist()\n",
    "temp_encoded = encode_genres(train_df, top_genres)\n",
    "genre_features = [col for col in temp_encoded.columns if col.startswith('genre_') and col != 'genre_simplified']\n",
    "\n",
    "# Full feature set\n",
    "features_all = numerical_features + genre_features + engineered_features\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Numerical features:  {len(numerical_features)}\")\n",
    "print(f\"Genre features:      {len(genre_features)}\")\n",
    "print(f\"Engineered features: {len(engineered_features)}\")\n",
    "print(f\"{'─'*30}\")\n",
    "print(f\"TOTAL:               {len(features_all)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Definitions\n",
    "\n",
    "Let's define all the models we want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test\n",
    "models = {}\n",
    "\n",
    "# ============================================\n",
    "# PREVIOUS BEST (for comparison)\n",
    "# ============================================\n",
    "models['ElasticNet (baseline)'] = {\n",
    "    'model': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42),\n",
    "    'scale': True,\n",
    "    'type': 'Linear'\n",
    "}\n",
    "\n",
    "# ============================================\n",
    "# SUPPORT VECTOR REGRESSION\n",
    "# ============================================\n",
    "models['SVR (RBF)'] = {\n",
    "    'model': SVR(kernel='rbf', C=10, gamma='scale', epsilon=0.1),\n",
    "    'scale': True,\n",
    "    'type': 'SVM'\n",
    "}\n",
    "\n",
    "models['SVR (Linear)'] = {\n",
    "    'model': SVR(kernel='linear', C=1),\n",
    "    'scale': True,\n",
    "    'type': 'SVM'\n",
    "}\n",
    "\n",
    "models['SVR (Poly)'] = {\n",
    "    'model': SVR(kernel='poly', degree=2, C=1, gamma='scale'),\n",
    "    'scale': True,\n",
    "    'type': 'SVM'\n",
    "}\n",
    "\n",
    "# ============================================\n",
    "# K-NEAREST NEIGHBORS\n",
    "# ============================================\n",
    "models['KNN (k=5)'] = {\n",
    "    'model': KNeighborsRegressor(n_neighbors=5, weights='distance', metric='euclidean'),\n",
    "    'scale': True,\n",
    "    'type': 'Distance-based'\n",
    "}\n",
    "\n",
    "models['KNN (k=10)'] = {\n",
    "    'model': KNeighborsRegressor(n_neighbors=10, weights='distance', metric='euclidean'),\n",
    "    'scale': True,\n",
    "    'type': 'Distance-based'\n",
    "}\n",
    "\n",
    "models['KNN (k=15)'] = {\n",
    "    'model': KNeighborsRegressor(n_neighbors=15, weights='distance', metric='euclidean'),\n",
    "    'scale': True,\n",
    "    'type': 'Distance-based'\n",
    "}\n",
    "\n",
    "# ============================================\n",
    "# BAYESIAN RIDGE\n",
    "# ============================================\n",
    "models['Bayesian Ridge'] = {\n",
    "    'model': BayesianRidge(alpha_1=1e-6, alpha_2=1e-6, lambda_1=1e-6, lambda_2=1e-6),\n",
    "    'scale': True,\n",
    "    'type': 'Bayesian'\n",
    "}\n",
    "\n",
    "# ============================================\n",
    "# XGBOOST\n",
    "# ============================================\n",
    "if XGB_AVAILABLE:\n",
    "    models['XGBoost'] = {\n",
    "        'model': xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=42,\n",
    "            verbosity=0\n",
    "        ),\n",
    "        'scale': False,\n",
    "        'type': 'Boosting'\n",
    "    }\n",
    "    \n",
    "    models['XGBoost (tuned)'] = {\n",
    "        'model': xgb.XGBRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=3,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            reg_alpha=0.5,\n",
    "            reg_lambda=2.0,\n",
    "            min_child_weight=3,\n",
    "            random_state=42,\n",
    "            verbosity=0\n",
    "        ),\n",
    "        'scale': False,\n",
    "        'type': 'Boosting'\n",
    "    }\n",
    "\n",
    "# ============================================\n",
    "# LIGHTGBM\n",
    "# ============================================\n",
    "if LGB_AVAILABLE:\n",
    "    models['LightGBM'] = {\n",
    "        'model': lgb.LGBMRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=42,\n",
    "            verbosity=-1\n",
    "        ),\n",
    "        'scale': False,\n",
    "        'type': 'Boosting'\n",
    "    }\n",
    "    \n",
    "    models['LightGBM (tuned)'] = {\n",
    "        'model': lgb.LGBMRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=3,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            reg_alpha=0.5,\n",
    "            reg_lambda=2.0,\n",
    "            min_child_samples=10,\n",
    "            random_state=42,\n",
    "            verbosity=-1\n",
    "        ),\n",
    "        'scale': False,\n",
    "        'type': 'Boosting'\n",
    "    }\n",
    "\n",
    "print(f\"Total models to test: {len(models)}\")\n",
    "print(\"\\nModels by type:\")\n",
    "for name, config in models.items():\n",
    "    print(f\"  - {name} ({config['type']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Run Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MODEL COMPARISON WITH LEAKAGE-SAFE 5-FOLD CV\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDataset: {len(train_df)} samples, {len(features_all)} features\")\n",
    "print(f\"Previous best (ElasticNet): CV RMSE = 10.42\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, config in models.items():\n",
    "    print(f\"\\nEvaluating: {name}...\")\n",
    "    \n",
    "    # Clone the model to avoid state issues\n",
    "    from sklearn.base import clone\n",
    "    model = clone(config['model'])\n",
    "    \n",
    "    result = full_pipeline_cv(\n",
    "        train_df, \n",
    "        features_all, \n",
    "        model, \n",
    "        scale=config['scale'], \n",
    "        cv=5,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    result['Model'] = name\n",
    "    result['Type'] = config['type']\n",
    "    result['Scale'] = config['scale']\n",
    "    results.append(result)\n",
    "    \n",
    "    # Color coding for results\n",
    "    if result['cv_rmse'] < 10.42:\n",
    "        status = \"NEW BEST!\"\n",
    "    elif result['cv_rmse'] < 10.50:\n",
    "        status = \"Competitive\"\n",
    "    else:\n",
    "        status = \"\"\n",
    "    \n",
    "    print(f\"  CV RMSE: {result['cv_rmse']:.4f} (+/- {result['cv_rmse_std']:.4f}) {status}\")\n",
    "    print(f\"  CV R²:   {result['cv_r2']:.4f} (+/- {result['cv_r2_std']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)[['Model', 'Type', 'cv_rmse', 'cv_rmse_std', 'cv_r2', 'cv_r2_std']]\n",
    "results_df = results_df.sort_values('cv_rmse').reset_index(drop=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RESULTS SUMMARY (sorted by CV RMSE)\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.round(4).to_string(index=False))\n",
    "\n",
    "# Highlight best\n",
    "best_model = results_df.iloc[0]\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BEST MODEL: {best_model['Model']}\")\n",
    "print(f\"CV RMSE: {best_model['cv_rmse']:.4f}\")\n",
    "print(f\"CV R²: {best_model['cv_r2']:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Model Comparison Bar Chart\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Color by model type\n",
    "type_colors = {\n",
    "    'Linear': '#3498db',\n",
    "    'SVM': '#e74c3c',\n",
    "    'Distance-based': '#2ecc71',\n",
    "    'Bayesian': '#9b59b6',\n",
    "    'Boosting': '#f39c12'\n",
    "}\n",
    "\n",
    "colors = [type_colors.get(t, 'gray') for t in results_df['Type']]\n",
    "\n",
    "# Highlight best model\n",
    "colors[0] = '#1a5c1a'  # Dark green for best\n",
    "\n",
    "bars = ax.barh(results_df['Model'], results_df['cv_rmse'], \n",
    "               xerr=results_df['cv_rmse_std'], capsize=4,\n",
    "               color=colors, edgecolor='black', linewidth=1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, results_df['cv_rmse']):\n",
    "    ax.text(val + 0.05, bar.get_y() + bar.get_height()/2, f'{val:.3f}',\n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Reference line for previous best\n",
    "ax.axvline(x=10.42, color='red', linestyle='--', linewidth=2, label='Previous Best (10.42)')\n",
    "\n",
    "ax.set_xlabel('CV RMSE (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Alternative Models Comparison\\n(5-Fold CV, Leakage-Safe Pipeline)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(9.5, max(results_df['cv_rmse']) + 0.5)\n",
    "\n",
    "# Legend for model types\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=c, edgecolor='black', label=t) for t, c in type_colors.items()]\n",
    "legend_elements.append(plt.Line2D([0], [0], color='red', linestyle='--', linewidth=2, label='Previous Best'))\n",
    "ax.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/alternative_models_comparison.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved: figures/alternative_models_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Model Type Comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Group by type and get best from each\n",
    "type_best = results_df.groupby('Type').agg({\n",
    "    'cv_rmse': 'min',\n",
    "    'Model': 'first'\n",
    "}).reset_index().sort_values('cv_rmse')\n",
    "\n",
    "colors = [type_colors.get(t, 'gray') for t in type_best['Type']]\n",
    "\n",
    "bars = ax.bar(type_best['Type'], type_best['cv_rmse'], color=colors, edgecolor='black', linewidth=2)\n",
    "\n",
    "for bar, val, model in zip(bars, type_best['cv_rmse'], type_best['Model']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05, \n",
    "            f'{val:.3f}\\n({model.split(\" \")[0]})',\n",
    "            ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.axhline(y=10.42, color='red', linestyle='--', linewidth=2, label='Previous Best (10.42)')\n",
    "\n",
    "ax.set_ylabel('Best CV RMSE', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Best Model from Each Category', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(9.5, max(type_best['cv_rmse']) + 0.5)\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/model_type_comparison.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved: figures/model_type_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Hyperparameter Tuning for Top Performers\n",
    "\n",
    "Let's tune the best performing models to see if we can squeeze out more performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify top 3 models (excluding baseline)\n",
    "top_models = results_df[results_df['Model'] != 'ElasticNet (baseline)'].head(3)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TOP 3 NEW MODELS (candidates for tuning)\")\n",
    "print(\"=\"*60)\n",
    "print(top_models[['Model', 'cv_rmse', 'cv_r2']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune SVR if it's a top performer\n",
    "if 'SVR' in results_df.iloc[0]['Model'] or 'SVR' in results_df.iloc[1]['Model']:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"HYPERPARAMETER TUNING: SVR (RBF)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    svr_params = {\n",
    "        'C': [0.1, 1, 10, 50, 100],\n",
    "        'gamma': ['scale', 'auto', 0.01, 0.1],\n",
    "        'epsilon': [0.01, 0.1, 0.5]\n",
    "    }\n",
    "    \n",
    "    print(f\"Testing {len(svr_params['C']) * len(svr_params['gamma']) * len(svr_params['epsilon'])} combinations...\")\n",
    "    \n",
    "    best_svr_rmse = float('inf')\n",
    "    best_svr_params = None\n",
    "    \n",
    "    for C in svr_params['C']:\n",
    "        for gamma in svr_params['gamma']:\n",
    "            for epsilon in svr_params['epsilon']:\n",
    "                model = SVR(kernel='rbf', C=C, gamma=gamma, epsilon=epsilon)\n",
    "                result = full_pipeline_cv(train_df, features_all, model, scale=True, cv=5)\n",
    "                \n",
    "                if result['cv_rmse'] < best_svr_rmse:\n",
    "                    best_svr_rmse = result['cv_rmse']\n",
    "                    best_svr_params = {'C': C, 'gamma': gamma, 'epsilon': epsilon}\n",
    "                    print(f\"  New best: C={C}, gamma={gamma}, epsilon={epsilon} -> RMSE={result['cv_rmse']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBest SVR params: {best_svr_params}\")\n",
    "    print(f\"Best SVR RMSE: {best_svr_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune KNN if it's a top performer\n",
    "if any('KNN' in m for m in results_df.head(3)['Model'].values):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"HYPERPARAMETER TUNING: KNN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    knn_params = {\n",
    "        'n_neighbors': [3, 5, 7, 10, 15, 20, 25, 30],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    }\n",
    "    \n",
    "    print(f\"Testing {len(knn_params['n_neighbors']) * len(knn_params['weights']) * len(knn_params['metric'])} combinations...\")\n",
    "    \n",
    "    best_knn_rmse = float('inf')\n",
    "    best_knn_params = None\n",
    "    \n",
    "    for k in knn_params['n_neighbors']:\n",
    "        for weights in knn_params['weights']:\n",
    "            for metric in knn_params['metric']:\n",
    "                model = KNeighborsRegressor(n_neighbors=k, weights=weights, metric=metric)\n",
    "                result = full_pipeline_cv(train_df, features_all, model, scale=True, cv=5)\n",
    "                \n",
    "                if result['cv_rmse'] < best_knn_rmse:\n",
    "                    best_knn_rmse = result['cv_rmse']\n",
    "                    best_knn_params = {'n_neighbors': k, 'weights': weights, 'metric': metric}\n",
    "                    print(f\"  New best: k={k}, weights={weights}, metric={metric} -> RMSE={result['cv_rmse']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBest KNN params: {best_knn_params}\")\n",
    "    print(f\"Best KNN RMSE: {best_knn_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune XGBoost if available and top performer\n",
    "if XGB_AVAILABLE and any('XGBoost' in m for m in results_df.head(5)['Model'].values):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"HYPERPARAMETER TUNING: XGBoost\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    xgb_params = {\n",
    "        'max_depth': [2, 3, 4, 5],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
    "        'reg_lambda': [1, 2, 5]\n",
    "    }\n",
    "    \n",
    "    # Simplified search (subset of combinations)\n",
    "    print(\"Running simplified grid search...\")\n",
    "    \n",
    "    best_xgb_rmse = float('inf')\n",
    "    best_xgb_params = None\n",
    "    \n",
    "    for max_depth in [2, 3, 4]:\n",
    "        for lr in [0.03, 0.05, 0.1]:\n",
    "            for reg_alpha in [0.1, 0.5, 1.0]:\n",
    "                model = xgb.XGBRegressor(\n",
    "                    n_estimators=200,\n",
    "                    max_depth=max_depth,\n",
    "                    learning_rate=lr,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    reg_alpha=reg_alpha,\n",
    "                    reg_lambda=2.0,\n",
    "                    random_state=42,\n",
    "                    verbosity=0\n",
    "                )\n",
    "                result = full_pipeline_cv(train_df, features_all, model, scale=False, cv=5)\n",
    "                \n",
    "                if result['cv_rmse'] < best_xgb_rmse:\n",
    "                    best_xgb_rmse = result['cv_rmse']\n",
    "                    best_xgb_params = {'max_depth': max_depth, 'learning_rate': lr, 'reg_alpha': reg_alpha}\n",
    "                    print(f\"  New best: depth={max_depth}, lr={lr}, alpha={reg_alpha} -> RMSE={result['cv_rmse']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBest XGBoost params: {best_xgb_params}\")\n",
    "    print(f\"Best XGBoost RMSE: {best_xgb_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Final Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all tuned results\n",
    "final_results = []\n",
    "\n",
    "# Add baseline ElasticNet\n",
    "final_results.append({\n",
    "    'Model': 'ElasticNet (Approach 3 v2)',\n",
    "    'cv_rmse': 10.42,\n",
    "    'Status': 'Previous Best'\n",
    "})\n",
    "\n",
    "# Add best from each new model type\n",
    "for _, row in results_df.iterrows():\n",
    "    if row['Model'] != 'ElasticNet (baseline)':\n",
    "        final_results.append({\n",
    "            'Model': row['Model'],\n",
    "            'cv_rmse': row['cv_rmse'],\n",
    "            'Status': 'New' if row['cv_rmse'] < 10.42 else 'Tested'\n",
    "        })\n",
    "\n",
    "final_df = pd.DataFrame(final_results).sort_values('cv_rmse').head(10)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL LEADERBOARD\")\n",
    "print(\"=\"*70)\n",
    "print(final_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Generate Submissions for Top Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(model, model_name, scale=True):\n",
    "    \"\"\"\n",
    "    Train on full training data and generate test predictions.\n",
    "    \"\"\"\n",
    "    # Learn top genres from full training data\n",
    "    final_top_genres = train_df['top genre'].value_counts().head(TOP_N_GENRES).index.tolist()\n",
    "    \n",
    "    # Encode\n",
    "    full_train_encoded = encode_genres(train_df, final_top_genres)\n",
    "    final_test_encoded = encode_genres(test_df, final_top_genres)\n",
    "    \n",
    "    # Align columns\n",
    "    final_genre_cols = [c for c in full_train_encoded.columns if c.startswith('genre_') and c != 'genre_simplified']\n",
    "    for col in final_genre_cols:\n",
    "        if col not in final_test_encoded.columns:\n",
    "            final_test_encoded[col] = 0\n",
    "    \n",
    "    # Feature engineering\n",
    "    full_train_fe = engineer_features(full_train_encoded)\n",
    "    final_test_fe = engineer_features(final_test_encoded)\n",
    "    \n",
    "    # Final feature set\n",
    "    final_features = numerical_features + final_genre_cols + engineered_features\n",
    "    \n",
    "    X_full = full_train_fe[final_features].copy()\n",
    "    X_test_final = final_test_fe[final_features].copy()\n",
    "    y_full = full_train_fe['pop']\n",
    "    \n",
    "    # Ensure columns match\n",
    "    for col in final_features:\n",
    "        if col not in X_test_final.columns:\n",
    "            X_test_final[col] = 0\n",
    "    X_test_final = X_test_final[final_features]\n",
    "    \n",
    "    # Scale if needed\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_full = pd.DataFrame(scaler.fit_transform(X_full), columns=X_full.columns, index=X_full.index)\n",
    "        X_test_final = pd.DataFrame(scaler.transform(X_test_final), columns=X_test_final.columns, index=X_test_final.index)\n",
    "    \n",
    "    # Train final model\n",
    "    model.fit(X_full.values, y_full.values)\n",
    "    \n",
    "    # Generate predictions\n",
    "    test_predictions = model.predict(X_test_final.values)\n",
    "    \n",
    "    # Create submission\n",
    "    submission = pd.DataFrame({\n",
    "        'Id': final_test_fe['Id'],\n",
    "        'pop': test_predictions\n",
    "    })\n",
    "    \n",
    "    # Clean filename\n",
    "    clean_name = model_name.lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    filename = f'./submission_approach5_{clean_name}.csv'\n",
    "    submission.to_csv(filename, index=False)\n",
    "    \n",
    "    return filename, test_predictions\n",
    "\n",
    "print(\"Submission function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submissions for top models\n",
    "print(\"=\"*70)\n",
    "print(\"GENERATING SUBMISSIONS FOR TOP MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "submissions = []\n",
    "\n",
    "# Get top 5 models\n",
    "top_5 = results_df.head(5)\n",
    "\n",
    "for _, row in top_5.iterrows():\n",
    "    model_name = row['Model']\n",
    "    model_config = models[model_name]\n",
    "    \n",
    "    print(f\"\\nGenerating: {model_name}...\")\n",
    "    \n",
    "    # Clone model\n",
    "    from sklearn.base import clone\n",
    "    model = clone(model_config['model'])\n",
    "    \n",
    "    filename, predictions = generate_submission(model, model_name, scale=model_config['scale'])\n",
    "    \n",
    "    submissions.append({\n",
    "        'Model': model_name,\n",
    "        'CV RMSE': row['cv_rmse'],\n",
    "        'File': filename,\n",
    "        'Pred Mean': predictions.mean(),\n",
    "        'Pred Std': predictions.std()\n",
    "    })\n",
    "    \n",
    "    print(f\"  Saved: {filename}\")\n",
    "    print(f\"  Predictions: mean={predictions.mean():.2f}, std={predictions.std():.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUBMISSION FILES CREATED\")\n",
    "print(\"=\"*70)\n",
    "submissions_df = pd.DataFrame(submissions)\n",
    "print(submissions_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Ensemble: Combine Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ensemble of top performers\n",
    "print(\"=\"*70)\n",
    "print(\"ENSEMBLE: COMBINING TOP MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get predictions from top 3 models for blending\n",
    "top_3_names = results_df.head(3)['Model'].values\n",
    "print(f\"\\nBlending predictions from: {list(top_3_names)}\")\n",
    "\n",
    "# Generate predictions for each\n",
    "ensemble_preds = {}\n",
    "\n",
    "for model_name in top_3_names:\n",
    "    model_config = models[model_name]\n",
    "    from sklearn.base import clone\n",
    "    model = clone(model_config['model'])\n",
    "    _, preds = generate_submission(model, model_name, scale=model_config['scale'])\n",
    "    ensemble_preds[model_name] = preds\n",
    "    print(f\"  {model_name}: mean={preds.mean():.2f}\")\n",
    "\n",
    "# Simple average blend\n",
    "blend_avg = np.mean([ensemble_preds[m] for m in top_3_names], axis=0)\n",
    "\n",
    "# Weighted blend (inversely proportional to CV RMSE)\n",
    "rmses = results_df.head(3)['cv_rmse'].values\n",
    "weights = 1 / rmses\n",
    "weights = weights / weights.sum()  # Normalize\n",
    "\n",
    "blend_weighted = np.average([ensemble_preds[m] for m in top_3_names], axis=0, weights=weights)\n",
    "\n",
    "print(f\"\\nBlend weights (inverse RMSE): {dict(zip(top_3_names, weights.round(3)))}\")\n",
    "print(f\"\\nSimple Average: mean={blend_avg.mean():.2f}, std={blend_avg.std():.2f}\")\n",
    "print(f\"Weighted Blend: mean={blend_weighted.mean():.2f}, std={blend_weighted.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ensemble submissions\n",
    "# Load test IDs\n",
    "test_ids = test_df['Id']\n",
    "\n",
    "# Simple average blend\n",
    "submission_avg = pd.DataFrame({'Id': test_ids, 'pop': blend_avg})\n",
    "submission_avg.to_csv('./submission_approach5_ensemble_avg.csv', index=False)\n",
    "print(\"Saved: submission_approach5_ensemble_avg.csv\")\n",
    "\n",
    "# Weighted blend\n",
    "submission_weighted = pd.DataFrame({'Id': test_ids, 'pop': blend_weighted})\n",
    "submission_weighted.to_csv('./submission_approach5_ensemble_weighted.csv', index=False)\n",
    "print(\"Saved: submission_approach5_ensemble_weighted.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: All approaches comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "all_approaches = [\n",
    "    ('Baseline (no genre)', 11.27, 'Previous'),\n",
    "    ('Approach 1 (Top 15 genres)', 10.93, 'Previous'),\n",
    "    ('Approach 2 (Hybrid genre)', 11.05, 'Previous'),\n",
    "    ('Approach 3 v1 (Feature eng)', 10.80, 'Previous'),\n",
    "    ('Approach 3 v2 (Leakage-safe)', 10.42, 'Previous Best'),\n",
    "    ('Approach 4 (Ensemble)', 10.43, 'Previous'),\n",
    "]\n",
    "\n",
    "# Add top 3 from this approach\n",
    "for _, row in results_df.head(3).iterrows():\n",
    "    all_approaches.append((f\"Approach 5: {row['Model']}\", row['cv_rmse'], 'New'))\n",
    "\n",
    "# Sort by RMSE\n",
    "all_approaches.sort(key=lambda x: x[1])\n",
    "\n",
    "names = [a[0] for a in all_approaches]\n",
    "rmses = [a[1] for a in all_approaches]\n",
    "categories = [a[2] for a in all_approaches]\n",
    "\n",
    "colors = []\n",
    "for cat in categories:\n",
    "    if cat == 'New':\n",
    "        colors.append('#27ae60')  # Green for new\n",
    "    elif cat == 'Previous Best':\n",
    "        colors.append('#f39c12')  # Orange for previous best\n",
    "    else:\n",
    "        colors.append('#3498db')  # Blue for previous\n",
    "\n",
    "bars = ax.barh(names, rmses, color=colors, edgecolor='black', linewidth=1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, rmses):\n",
    "    ax.text(val + 0.02, bar.get_y() + bar.get_height()/2, f'{val:.2f}',\n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('CV RMSE (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('All Approaches Comparison\\nSpotify Popularity Prediction', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(10, 11.5)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#27ae60', edgecolor='black', label='New (Approach 5)'),\n",
    "    Patch(facecolor='#f39c12', edgecolor='black', label='Previous Best'),\n",
    "    Patch(facecolor='#3498db', edgecolor='black', label='Previous Approaches')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/all_approaches_comparison.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved: figures/all_approaches_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"APPROACH 5 SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n MODELS TESTED:\")\n",
    "print(\"  - SVR (RBF, Linear, Polynomial kernels)\")\n",
    "print(\"  - KNN Regression (k=5, 10, 15)\")\n",
    "print(\"  - Bayesian Ridge\")\n",
    "if XGB_AVAILABLE:\n",
    "    print(\"  - XGBoost (default and tuned)\")\n",
    "if LGB_AVAILABLE:\n",
    "    print(\"  - LightGBM (default and tuned)\")\n",
    "\n",
    "print(\"\\n TOP PERFORMERS:\")\n",
    "for i, row in results_df.head(5).iterrows():\n",
    "    status = \" (BEATS BASELINE!)\" if row['cv_rmse'] < 10.42 else \"\"\n",
    "    print(f\"  {i+1}. {row['Model']}: {row['cv_rmse']:.4f}{status}\")\n",
    "\n",
    "best = results_df.iloc[0]\n",
    "print(f\"\\n BEST MODEL: {best['Model']}\")\n",
    "print(f\"   CV RMSE: {best['cv_rmse']:.4f}\")\n",
    "print(f\"   CV R²: {best['cv_r2']:.4f}\")\n",
    "\n",
    "print(\"\\n SUBMISSIONS GENERATED:\")\n",
    "print(\"  - Individual model submissions (top 5)\")\n",
    "print(\"  - Ensemble average blend\")\n",
    "print(\"  - Ensemble weighted blend\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **SVR with RBF kernel** can be competitive on small datasets\n",
    "   - Captures non-linear relationships\n",
    "   - Requires careful tuning of C, gamma, and epsilon\n",
    "\n",
    "2. **KNN Regression** provides a simple but effective baseline\n",
    "   - Works well when similar songs have similar popularity\n",
    "   - Sensitive to the choice of k and distance metric\n",
    "\n",
    "3. **XGBoost/LightGBM** need careful regularization on small datasets\n",
    "   - Prone to overfitting with default parameters\n",
    "   - Low max_depth and high regularization help\n",
    "\n",
    "4. **Bayesian Ridge** automatically tunes regularization\n",
    "   - Good for small datasets with uncertainty\n",
    "\n",
    "### Recommendations for Kaggle:\n",
    "\n",
    "Try submitting in this order:\n",
    "1. Best individual model from this notebook\n",
    "2. Weighted ensemble blend\n",
    "3. ElasticNet (previous best) if new models don't improve\n",
    "\n",
    "Remember: CV score doesn't guarantee Kaggle score!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
