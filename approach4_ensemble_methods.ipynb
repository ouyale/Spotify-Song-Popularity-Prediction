{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 4: Ensemble Methods\n",
    "\n",
    "## Spotify Song Popularity Prediction\n",
    "\n",
    "In the previous approaches, we found that **ElasticNet** achieved the best cross-validation RMSE (10.42). However, when we submitted predictions to Kaggle, the score didn't improve as expected.\n",
    "\n",
    "**Why?** Different models capture different patterns in data:\n",
    "- **Linear models** (Ridge, Lasso, ElasticNet) assume linear relationships\n",
    "- **Tree-based models** (Random Forest, Gradient Boosting) capture non-linear patterns\n",
    "\n",
    "**Solution:** Combine multiple models using ensemble methods to leverage the strengths of each.\n",
    "\n",
    "---\n",
    "\n",
    "### What We'll Cover:\n",
    "1. Understanding ensemble methods (Voting, Stacking, Blending)\n",
    "2. Comparing single models vs ensembles\n",
    "3. Finding the best combination for Kaggle submission\n",
    "\n",
    "---\n",
    "\n",
    "### Previous Results:\n",
    "| Approach | Best Model | CV RMSE |\n",
    "|----------|------------|--------|\n",
    "| Baseline | - | 11.27 |\n",
    "| Approach 1 (top 15 genres) | Random Forest | 10.93 |\n",
    "| Approach 2 (hybrid groups) | Ridge | 11.05 |\n",
    "| Approach 3 v2 (leakage-safe) | ElasticNet | 10.42 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Understanding Ensemble Methods\n",
    "\n",
    "Before we dive into the code, let's understand the three main ensemble approaches we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ensemble methods concept\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. Voting\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.add_patch(plt.Rectangle((1, 6), 2, 2, color='steelblue', alpha=0.7))\n",
    "ax1.add_patch(plt.Rectangle((4, 6), 2, 2, color='coral', alpha=0.7))\n",
    "ax1.add_patch(plt.Rectangle((7, 6), 2, 2, color='green', alpha=0.7))\n",
    "ax1.text(2, 7, 'Model 1', ha='center', va='center', fontsize=10, color='white', fontweight='bold')\n",
    "ax1.text(5, 7, 'Model 2', ha='center', va='center', fontsize=10, color='white', fontweight='bold')\n",
    "ax1.text(8, 7, 'Model 3', ha='center', va='center', fontsize=10, color='white', fontweight='bold')\n",
    "ax1.annotate('', xy=(5, 4.5), xytext=(2, 5.8), arrowprops=dict(arrowstyle='->', color='black'))\n",
    "ax1.annotate('', xy=(5, 4.5), xytext=(5, 5.8), arrowprops=dict(arrowstyle='->', color='black'))\n",
    "ax1.annotate('', xy=(5, 4.5), xytext=(8, 5.8), arrowprops=dict(arrowstyle='->', color='black'))\n",
    "ax1.add_patch(plt.Rectangle((3.5, 2.5), 3, 2, color='purple', alpha=0.7))\n",
    "ax1.text(5, 3.5, 'AVERAGE', ha='center', va='center', fontsize=10, color='white', fontweight='bold')\n",
    "ax1.annotate('', xy=(5, 1), xytext=(5, 2.3), arrowprops=dict(arrowstyle='->', color='black'))\n",
    "ax1.add_patch(plt.Rectangle((3.5, 0), 3, 1, color='gold', alpha=0.9))\n",
    "ax1.text(5, 0.5, 'Final Prediction', ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "ax1.set_title('VOTING\\n(Simple Average)', fontsize=12, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# 2. Stacking\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.add_patch(plt.Rectangle((1, 6), 2, 2, color='steelblue', alpha=0.7))\n",
    "ax2.add_patch(plt.Rectangle((4, 6), 2, 2, color='coral', alpha=0.7))\n",
    "ax2.add_patch(plt.Rectangle((7, 6), 2, 2, color='green', alpha=0.7))\n",
    "ax2.text(2, 7, 'Model 1', ha='center', va='center', fontsize=10, color='white', fontweight='bold')\n",
    "ax2.text(5, 7, 'Model 2', ha='center', va='center', fontsize=10, color='white', fontweight='bold')\n",
    "ax2.text(8, 7, 'Model 3', ha='center', va='center', fontsize=10, color='white', fontweight='bold')\n",
    "ax2.annotate('', xy=(5, 4.5), xytext=(2, 5.8), arrowprops=dict(arrowstyle='->', color='black'))\n",
    "ax2.annotate('', xy=(5, 4.5), xytext=(5, 5.8), arrowprops=dict(arrowstyle='->', color='black'))\n",
    "ax2.annotate('', xy=(5, 4.5), xytext=(8, 5.8), arrowprops=dict(arrowstyle='->', color='black'))\n",
    "ax2.add_patch(plt.Rectangle((3, 2.5), 4, 2, color='darkred', alpha=0.7))\n",
    "ax2.text(5, 3.5, 'META-LEARNER', ha='center', va='center', fontsize=9, color='white', fontweight='bold')\n",
    "ax2.annotate('', xy=(5, 1), xytext=(5, 2.3), arrowprops=dict(arrowstyle='->', color='black'))\n",
    "ax2.add_patch(plt.Rectangle((3.5, 0), 3, 1, color='gold', alpha=0.9))\n",
    "ax2.text(5, 0.5, 'Final Prediction', ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "ax2.set_title('STACKING\\n(Meta-Learner Combines)', fontsize=12, fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "# 3. Blending\n",
    "ax3 = axes[2]\n",
    "ax3.set_xlim(0, 10)\n",
    "ax3.set_ylim(0, 10)\n",
    "ax3.add_patch(plt.Rectangle((1, 6), 2, 2, color='steelblue', alpha=0.7))\n",
    "ax3.add_patch(plt.Rectangle((4, 6), 2, 2, color='coral', alpha=0.7))\n",
    "ax3.add_patch(plt.Rectangle((7, 6), 2, 2, color='green', alpha=0.7))\n",
    "ax3.text(2, 7, 'Model 1', ha='center', va='center', fontsize=10, color='white', fontweight='bold')\n",
    "ax3.text(5, 7, 'Model 2', ha='center', va='center', fontsize=10, color='white', fontweight='bold')\n",
    "ax3.text(8, 7, 'Model 3', ha='center', va='center', fontsize=10, color='white', fontweight='bold')\n",
    "ax3.text(2, 5.5, '30%', ha='center', fontsize=9, color='steelblue', fontweight='bold')\n",
    "ax3.text(5, 5.5, '50%', ha='center', fontsize=9, color='coral', fontweight='bold')\n",
    "ax3.text(8, 5.5, '20%', ha='center', fontsize=9, color='green', fontweight='bold')\n",
    "ax3.annotate('', xy=(5, 4.5), xytext=(2, 5.2), arrowprops=dict(arrowstyle='->', color='black'))\n",
    "ax3.annotate('', xy=(5, 4.5), xytext=(5, 5.2), arrowprops=dict(arrowstyle='->', color='black'))\n",
    "ax3.annotate('', xy=(5, 4.5), xytext=(8, 5.2), arrowprops=dict(arrowstyle='->', color='black'))\n",
    "ax3.add_patch(plt.Rectangle((2.5, 2.5), 5, 2, color='darkorange', alpha=0.7))\n",
    "ax3.text(5, 3.5, 'WEIGHTED AVG', ha='center', va='center', fontsize=9, color='white', fontweight='bold')\n",
    "ax3.annotate('', xy=(5, 1), xytext=(5, 2.3), arrowprops=dict(arrowstyle='->', color='black'))\n",
    "ax3.add_patch(plt.Rectangle((3.5, 0), 3, 1, color='gold', alpha=0.9))\n",
    "ax3.text(5, 0.5, 'Final Prediction', ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "ax3.set_title('BLENDING\\n(Weighted Average)', fontsize=12, fontweight='bold')\n",
    "ax3.axis('off')\n",
    "\n",
    "plt.suptitle('Three Ensemble Approaches', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods Explained:\n",
    "\n",
    "| Method | How It Works | Pros | Cons |\n",
    "|--------|--------------|------|------|\n",
    "| **Voting** | Averages predictions from all models equally | Simple, reduces variance | Treats all models equally |\n",
    "| **Stacking** | Uses another model to learn how to combine predictions | Can learn optimal combination | Risk of overfitting |\n",
    "| **Blending** | Weighted average with custom weights | Control over each model's contribution | Need to tune weights |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load and Prepare Data\n",
    "\n",
    "We'll use the same leakage-safe methodology from Approach 3 v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('./data/CS98XRegressionTrain.csv')\n",
    "test_df = pd.read_csv('./data/CS98XRegressionTest.csv')\n",
    "\n",
    "# Handle missing genres\n",
    "train_df['top genre'] = train_df['top genre'].fillna('Unknown').replace('', 'Unknown')\n",
    "test_df['top genre'] = test_df['top genre'].fillna('Unknown').replace('', 'Unknown')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA LOADED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training set: {train_df.shape[0]} rows, {train_df.shape[1]} columns\")\n",
    "print(f\"Test set:     {test_df.shape[0]} rows, {test_df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and feature definitions\n",
    "TOP_N_GENRES = 15\n",
    "numerical_features = ['bpm', 'nrgy', 'dnce', 'dB', 'live', 'val', 'dur', 'acous', 'spch']\n",
    "\n",
    "# Learn top genres from training data only\n",
    "top_genres = train_df['top genre'].value_counts().head(TOP_N_GENRES).index.tolist()\n",
    "\n",
    "print(f\"Top {TOP_N_GENRES} genres (learned from training data):\")\n",
    "for i, genre in enumerate(top_genres, 1):\n",
    "    print(f\"  {i:2}. {genre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_genres(df, top_genres):\n",
    "    \"\"\"One-hot encode genres, grouping rare genres as 'other'.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['genre_simplified'] = df['top genre'].apply(\n",
    "        lambda x: x if x in top_genres else 'other'\n",
    "    )\n",
    "    genre_dummies = pd.get_dummies(df['genre_simplified'], prefix='genre')\n",
    "    df = pd.concat([df, genre_dummies], axis=1)\n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create engineered features from numerical columns.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Interaction terms\n",
    "    df['nrgy_x_dnce'] = df['nrgy'] * df['dnce']\n",
    "    df['nrgy_x_val'] = df['nrgy'] * df['val']\n",
    "    df['nrgy_x_dB'] = df['nrgy'] * df['dB']\n",
    "    df['dnce_x_val'] = df['dnce'] * df['val']\n",
    "    df['dnce_x_bpm'] = df['dnce'] * df['bpm']\n",
    "    df['acous_x_nrgy'] = df['acous'] * df['nrgy']\n",
    "    \n",
    "    # Ratios\n",
    "    df['nrgy_per_bpm'] = df['nrgy'] / (df['bpm'] + 1)\n",
    "    df['dnce_per_nrgy'] = df['dnce'] / (df['nrgy'] + 1)\n",
    "    df['val_per_nrgy'] = df['val'] / (df['nrgy'] + 1)\n",
    "    df['spch_per_dur'] = df['spch'] / (df['dur'] + 1)\n",
    "    \n",
    "    # Polynomial features\n",
    "    df['dur_squared'] = df['dur'] ** 2\n",
    "    df['acous_squared'] = df['acous'] ** 2\n",
    "    df['dB_squared'] = df['dB'] ** 2\n",
    "    df['nrgy_squared'] = df['nrgy'] ** 2\n",
    "    \n",
    "    # Binned features\n",
    "    df['bpm_slow'] = (df['bpm'] < 100).astype(int)\n",
    "    df['bpm_medium'] = ((df['bpm'] >= 100) & (df['bpm'] < 130)).astype(int)\n",
    "    df['bpm_fast'] = (df['bpm'] >= 130).astype(int)\n",
    "    df['low_energy'] = (df['nrgy'] < 50).astype(int)\n",
    "    df['high_energy'] = (df['nrgy'] >= 70).astype(int)\n",
    "    df['is_acoustic'] = (df['acous'] > 50).astype(int)\n",
    "    df['short_song'] = (df['dur'] < 180).astype(int)\n",
    "    df['long_song'] = (df['dur'] > 300).astype(int)\n",
    "    \n",
    "    # Composite scores\n",
    "    df['party_score'] = (df['nrgy'] + df['dnce'] + df['val'] - df['acous']) / 4\n",
    "    df['chill_score'] = (df['acous'] + (100 - df['nrgy']) + (100 - df['dnce'])) / 3\n",
    "    df['vocal_score'] = df['spch'] + df['live']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Engineered features list\n",
    "engineered_features = [\n",
    "    'nrgy_x_dnce', 'nrgy_x_val', 'nrgy_x_dB', 'dnce_x_val', 'dnce_x_bpm', 'acous_x_nrgy',\n",
    "    'nrgy_per_bpm', 'dnce_per_nrgy', 'val_per_nrgy', 'spch_per_dur',\n",
    "    'dur_squared', 'acous_squared', 'dB_squared', 'nrgy_squared',\n",
    "    'bpm_slow', 'bpm_medium', 'bpm_fast', 'low_energy', 'high_energy',\n",
    "    'is_acoustic', 'short_song', 'long_song',\n",
    "    'party_score', 'chill_score', 'vocal_score'\n",
    "]\n",
    "\n",
    "print(\"Feature engineering functions defined\")\n",
    "print(f\"  - {len(numerical_features)} numerical features\")\n",
    "print(f\"  - {len(engineered_features)} engineered features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply encoding and feature engineering\n",
    "train_enc = encode_genres(train_df, top_genres)\n",
    "test_enc = encode_genres(test_df, top_genres)\n",
    "\n",
    "train_fe = engineer_features(train_enc)\n",
    "test_fe = engineer_features(test_enc)\n",
    "\n",
    "# Get genre columns\n",
    "genre_cols = [c for c in train_fe.columns if c.startswith('genre_') and c != 'genre_simplified']\n",
    "\n",
    "# Align test columns with training\n",
    "for col in genre_cols:\n",
    "    if col not in test_fe.columns:\n",
    "        test_fe[col] = 0\n",
    "\n",
    "# Define complete feature set\n",
    "features = numerical_features + genre_cols + engineered_features\n",
    "\n",
    "# Prepare X and y\n",
    "X = train_fe[features].copy()\n",
    "y = train_fe['pop'].copy()\n",
    "X_test = test_fe[features].copy()\n",
    "\n",
    "# Ensure test has all features\n",
    "for col in features:\n",
    "    if col not in X_test.columns:\n",
    "        X_test[col] = 0\n",
    "X_test = X_test[features]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURES PREPARED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total features: {len(features)}\")\n",
    "print(f\"  - Numerical:   {len(numerical_features)}\")\n",
    "print(f\"  - Genre:       {len(genre_cols)}\")\n",
    "print(f\"  - Engineered:  {len(engineered_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature breakdown\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "feature_counts = {\n",
    "    'Numerical\\n(original)': len(numerical_features),\n",
    "    'Genre\\n(one-hot)': len(genre_cols),\n",
    "    'Engineered\\n(interactions, ratios, etc.)': len(engineered_features)\n",
    "}\n",
    "\n",
    "colors = ['steelblue', 'coral', 'green']\n",
    "bars = ax.bar(feature_counts.keys(), feature_counts.values(), color=colors, edgecolor='black')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, feature_counts.values()):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "            str(val), ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Number of Features', fontsize=12)\n",
    "ax.set_title(f'Feature Breakdown (Total: {len(features)} features)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, max(feature_counts.values()) + 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for linear models\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "print(\"Features scaled (for linear models)\")\n",
    "print(\"Note: Tree-based models will use unscaled features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Single Model Performance (Baseline)\n",
    "\n",
    "First, let's see how each individual model performs. This gives us a baseline to compare our ensembles against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models\n",
    "base_models = {\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=0.1),\n",
    "    'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "# Cross-validate each model\n",
    "print(\"=\"*60)\n",
    "print(\"SINGLE MODEL PERFORMANCE (5-fold CV)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "base_results = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in base_models.items():\n",
    "    cv_rmses = []\n",
    "    cv_r2s = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        # Use scaled data for linear models, raw for tree-based\n",
    "        if name in ['Ridge', 'Lasso', 'ElasticNet']:\n",
    "            X_tr, X_va = X_scaled.iloc[train_idx], X_scaled.iloc[val_idx]\n",
    "        else:\n",
    "            X_tr, X_va = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        \n",
    "        y_tr, y_va = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_tr.values, y_tr.values)\n",
    "        y_pred = model.predict(X_va.values)\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_va, y_pred))\n",
    "        r2 = r2_score(y_va, y_pred)\n",
    "        cv_rmses.append(rmse)\n",
    "        cv_r2s.append(r2)\n",
    "    \n",
    "    cv_rmse = np.mean(cv_rmses)\n",
    "    cv_std = np.std(cv_rmses)\n",
    "    cv_r2 = np.mean(cv_r2s)\n",
    "    \n",
    "    base_results.append({\n",
    "        'Model': name, \n",
    "        'CV RMSE': cv_rmse, \n",
    "        'Std': cv_std,\n",
    "        'CV R2': cv_r2,\n",
    "        'Type': 'Linear' if name in ['Ridge', 'Lasso', 'ElasticNet'] else 'Tree-based'\n",
    "    })\n",
    "    print(f\"{name:20} CV RMSE: {cv_rmse:.4f} (+/- {cv_std:.4f})  R2: {cv_r2:.3f}\")\n",
    "\n",
    "base_df = pd.DataFrame(base_results).sort_values('CV RMSE')\n",
    "print(f\"\\nBest single model: {base_df.iloc[0]['Model']} (RMSE: {base_df.iloc[0]['CV RMSE']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize single model performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: CV RMSE comparison\n",
    "ax1 = axes[0]\n",
    "colors = ['steelblue' if t == 'Linear' else 'coral' for t in base_df['Type']]\n",
    "bars = ax1.barh(base_df['Model'], base_df['CV RMSE'], xerr=base_df['Std'], \n",
    "                color=colors, edgecolor='black', capsize=5)\n",
    "ax1.set_xlabel('CV RMSE (lower is better)', fontsize=11)\n",
    "ax1.set_title('Single Model CV RMSE Comparison', fontsize=12, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars, base_df['CV RMSE'])):\n",
    "    ax1.text(val + 0.15, i, f'{val:.2f}', va='center', fontsize=10)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='steelblue', label='Linear Models'),\n",
    "                   Patch(facecolor='coral', label='Tree-based Models')]\n",
    "ax1.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "# Plot 2: Linear vs Tree-based comparison\n",
    "ax2 = axes[1]\n",
    "linear_avg = base_df[base_df['Type'] == 'Linear']['CV RMSE'].mean()\n",
    "tree_avg = base_df[base_df['Type'] == 'Tree-based']['CV RMSE'].mean()\n",
    "\n",
    "bars2 = ax2.bar(['Linear Models\\n(Ridge, Lasso, ElasticNet)', 'Tree-based Models\\n(RF, Gradient Boosting)'],\n",
    "                [linear_avg, tree_avg], color=['steelblue', 'coral'], edgecolor='black')\n",
    "ax2.set_ylabel('Average CV RMSE', fontsize=11)\n",
    "ax2.set_title('Linear vs Tree-based Models', fontsize=12, fontweight='bold')\n",
    "\n",
    "for bar, val in zip(bars2, [linear_avg, tree_avg]):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             f'{val:.2f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLinear models average RMSE: {linear_avg:.4f}\")\n",
    "print(f\"Tree-based models average RMSE: {tree_avg:.4f}\")\n",
    "print(f\"Linear models are {tree_avg - linear_avg:.4f} RMSE better on average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observation:\n",
    "Linear models (especially ElasticNet) outperform tree-based models on CV. But CV isn't everything - let's see if combining them helps on the actual test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Voting Ensembles\n",
    "\n",
    "Voting simply averages predictions from multiple models. It's the simplest ensemble method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define voting ensembles\n",
    "voting_ensembles = {\n",
    "    'Voting: Linear Only': VotingRegressor([\n",
    "        ('ridge', Ridge(alpha=1.0)),\n",
    "        ('lasso', Lasso(alpha=0.1)),\n",
    "        ('elasticnet', ElasticNet(alpha=0.1, l1_ratio=0.5))\n",
    "    ]),\n",
    "    \n",
    "    'Voting: Tree Only': VotingRegressor([\n",
    "        ('rf', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)),\n",
    "        ('gb', GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42))\n",
    "    ]),\n",
    "    \n",
    "    'Voting: Mixed (Ridge+RF+GB)': VotingRegressor([\n",
    "        ('ridge', Ridge(alpha=1.0)),\n",
    "        ('rf', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)),\n",
    "        ('gb', GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42))\n",
    "    ]),\n",
    "    \n",
    "    'Voting: All 5 Models': VotingRegressor([\n",
    "        ('ridge', Ridge(alpha=1.0)),\n",
    "        ('lasso', Lasso(alpha=0.1)),\n",
    "        ('elasticnet', ElasticNet(alpha=0.1, l1_ratio=0.5)),\n",
    "        ('rf', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)),\n",
    "        ('gb', GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VOTING ENSEMBLE PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "voting_results = []\n",
    "\n",
    "for name, model in voting_ensembles.items():\n",
    "    cv_rmses = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_scaled):\n",
    "        X_tr, X_va = X_scaled.iloc[train_idx], X_scaled.iloc[val_idx]\n",
    "        y_tr, y_va = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_tr.values, y_tr.values)\n",
    "        y_pred = model.predict(X_va.values)\n",
    "        rmse = np.sqrt(mean_squared_error(y_va, y_pred))\n",
    "        cv_rmses.append(rmse)\n",
    "    \n",
    "    cv_rmse = np.mean(cv_rmses)\n",
    "    cv_std = np.std(cv_rmses)\n",
    "    voting_results.append({'Ensemble': name, 'CV RMSE': cv_rmse, 'Std': cv_std, 'Type': 'Voting'})\n",
    "    print(f\"{name:30} CV RMSE: {cv_rmse:.4f} (+/- {cv_std:.4f})\")\n",
    "\n",
    "voting_df = pd.DataFrame(voting_results).sort_values('CV RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize voting results\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "y_pos = np.arange(len(voting_df))\n",
    "bars = ax.barh(y_pos, voting_df['CV RMSE'], xerr=voting_df['Std'], \n",
    "               color='purple', alpha=0.7, edgecolor='black', capsize=5)\n",
    "\n",
    "# Add best single model line\n",
    "best_single = base_df.iloc[0]['CV RMSE']\n",
    "ax.axvline(x=best_single, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Best Single Model ({best_single:.2f})')\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(voting_df['Ensemble'])\n",
    "ax.set_xlabel('CV RMSE (lower is better)', fontsize=11)\n",
    "ax.set_title('Voting Ensemble Performance', fontsize=12, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "# Add value labels\n",
    "for i, val in enumerate(voting_df['CV RMSE']):\n",
    "    ax.text(val + 0.1, i, f'{val:.2f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Stacking Ensembles\n",
    "\n",
    "Stacking uses a \"meta-learner\" to combine predictions from base models. The meta-learner learns the optimal way to combine predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stacking ensembles\n",
    "stacking_ensembles = {\n",
    "    'Stacking: RF+GB -> Ridge': StackingRegressor(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)),\n",
    "            ('gb', GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42))\n",
    "        ],\n",
    "        final_estimator=Ridge(alpha=1.0),\n",
    "        cv=5\n",
    "    ),\n",
    "    \n",
    "    'Stacking: Linear -> RF': StackingRegressor(\n",
    "        estimators=[\n",
    "            ('ridge', Ridge(alpha=1.0)),\n",
    "            ('lasso', Lasso(alpha=0.1)),\n",
    "            ('elasticnet', ElasticNet(alpha=0.1, l1_ratio=0.5))\n",
    "        ],\n",
    "        final_estimator=RandomForestRegressor(n_estimators=50, max_depth=5, random_state=42),\n",
    "        cv=5\n",
    "    ),\n",
    "    \n",
    "    'Stacking: All -> Ridge': StackingRegressor(\n",
    "        estimators=[\n",
    "            ('ridge', Ridge(alpha=1.0)),\n",
    "            ('elasticnet', ElasticNet(alpha=0.1, l1_ratio=0.5)),\n",
    "            ('rf', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)),\n",
    "            ('gb', GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42))\n",
    "        ],\n",
    "        final_estimator=Ridge(alpha=1.0),\n",
    "        cv=5\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STACKING ENSEMBLE PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "stacking_results = []\n",
    "\n",
    "for name, model in stacking_ensembles.items():\n",
    "    cv_rmses = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_scaled):\n",
    "        X_tr, X_va = X_scaled.iloc[train_idx], X_scaled.iloc[val_idx]\n",
    "        y_tr, y_va = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_tr.values, y_tr.values)\n",
    "        y_pred = model.predict(X_va.values)\n",
    "        rmse = np.sqrt(mean_squared_error(y_va, y_pred))\n",
    "        cv_rmses.append(rmse)\n",
    "    \n",
    "    cv_rmse = np.mean(cv_rmses)\n",
    "    cv_std = np.std(cv_rmses)\n",
    "    stacking_results.append({'Ensemble': name, 'CV RMSE': cv_rmse, 'Std': cv_std, 'Type': 'Stacking'})\n",
    "    print(f\"{name:30} CV RMSE: {cv_rmse:.4f} (+/- {cv_std:.4f})\")\n",
    "\n",
    "stacking_df = pd.DataFrame(stacking_results).sort_values('CV RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Blending (Weighted Average)\n",
    "\n",
    "Blending lets us assign custom weights to each model's predictions. This gives us fine-grained control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define blend configurations to test\n",
    "blends = {\n",
    "    'Equal Weight (All 5)': {\n",
    "        'Ridge': 0.2, 'Lasso': 0.2, 'ElasticNet': 0.2, \n",
    "        'RandomForest': 0.2, 'GradientBoosting': 0.2\n",
    "    },\n",
    "    'Linear Only (Equal)': {\n",
    "        'Ridge': 0.33, 'Lasso': 0.33, 'ElasticNet': 0.34, \n",
    "        'RandomForest': 0, 'GradientBoosting': 0\n",
    "    },\n",
    "    'Heavy ElasticNet': {\n",
    "        'Ridge': 0.15, 'Lasso': 0.15, 'ElasticNet': 0.5, \n",
    "        'RandomForest': 0.1, 'GradientBoosting': 0.1\n",
    "    },\n",
    "    'ElasticNet + RF (60/40)': {\n",
    "        'Ridge': 0, 'Lasso': 0, 'ElasticNet': 0.6, \n",
    "        'RandomForest': 0.4, 'GradientBoosting': 0\n",
    "    },\n",
    "    'ElasticNet + RF (70/30)': {\n",
    "        'Ridge': 0, 'Lasso': 0, 'ElasticNet': 0.7, \n",
    "        'RandomForest': 0.3, 'GradientBoosting': 0\n",
    "    },\n",
    "    'Ridge + ElasticNet': {\n",
    "        'Ridge': 0.5, 'Lasso': 0, 'ElasticNet': 0.5, \n",
    "        'RandomForest': 0, 'GradientBoosting': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BLENDING PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "blend_results = []\n",
    "\n",
    "for blend_name, weights in blends.items():\n",
    "    cv_rmses = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        # Prepare fold data\n",
    "        X_tr_scaled = X_scaled.iloc[train_idx]\n",
    "        X_va_scaled = X_scaled.iloc[val_idx]\n",
    "        X_tr_raw = X.iloc[train_idx]\n",
    "        X_va_raw = X.iloc[val_idx]\n",
    "        y_tr = y.iloc[train_idx]\n",
    "        y_va = y.iloc[val_idx]\n",
    "        \n",
    "        # Train models and get predictions\n",
    "        preds = {}\n",
    "        \n",
    "        if weights['Ridge'] > 0:\n",
    "            m = Ridge(alpha=1.0)\n",
    "            m.fit(X_tr_scaled.values, y_tr.values)\n",
    "            preds['Ridge'] = m.predict(X_va_scaled.values)\n",
    "        \n",
    "        if weights['Lasso'] > 0:\n",
    "            m = Lasso(alpha=0.1)\n",
    "            m.fit(X_tr_scaled.values, y_tr.values)\n",
    "            preds['Lasso'] = m.predict(X_va_scaled.values)\n",
    "        \n",
    "        if weights['ElasticNet'] > 0:\n",
    "            m = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "            m.fit(X_tr_scaled.values, y_tr.values)\n",
    "            preds['ElasticNet'] = m.predict(X_va_scaled.values)\n",
    "        \n",
    "        if weights['RandomForest'] > 0:\n",
    "            m = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "            m.fit(X_tr_raw.values, y_tr.values)\n",
    "            preds['RandomForest'] = m.predict(X_va_raw.values)\n",
    "        \n",
    "        if weights['GradientBoosting'] > 0:\n",
    "            m = GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "            m.fit(X_tr_raw.values, y_tr.values)\n",
    "            preds['GradientBoosting'] = m.predict(X_va_raw.values)\n",
    "        \n",
    "        # Blend predictions\n",
    "        blend_pred = sum(weights[name] * preds[name] for name in preds)\n",
    "        rmse = np.sqrt(mean_squared_error(y_va, blend_pred))\n",
    "        cv_rmses.append(rmse)\n",
    "    \n",
    "    cv_rmse = np.mean(cv_rmses)\n",
    "    cv_std = np.std(cv_rmses)\n",
    "    blend_results.append({'Ensemble': blend_name, 'CV RMSE': cv_rmse, 'Std': cv_std, 'Type': 'Blending'})\n",
    "    print(f\"{blend_name:30} CV RMSE: {cv_rmse:.4f} (+/- {cv_std:.4f})\")\n",
    "\n",
    "blend_df = pd.DataFrame(blend_results).sort_values('CV RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize blend weights for top blends\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors_map = {\n",
    "    'Ridge': 'steelblue',\n",
    "    'Lasso': 'lightblue', \n",
    "    'ElasticNet': 'navy',\n",
    "    'RandomForest': 'coral',\n",
    "    'GradientBoosting': 'lightsalmon'\n",
    "}\n",
    "\n",
    "for i, (blend_name, weights) in enumerate(blends.items()):\n",
    "    ax = axes[i]\n",
    "    active_models = {k: v for k, v in weights.items() if v > 0}\n",
    "    \n",
    "    colors = [colors_map[m] for m in active_models.keys()]\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        active_models.values(), \n",
    "        labels=active_models.keys(),\n",
    "        autopct='%1.0f%%',\n",
    "        colors=colors,\n",
    "        startangle=90\n",
    "    )\n",
    "    ax.set_title(blend_name, fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Blend Weight Distributions', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Complete Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = []\n",
    "\n",
    "# Single models\n",
    "for r in base_results:\n",
    "    all_results.append({\n",
    "        'Method': r['Model'], \n",
    "        'Type': 'Single Model', \n",
    "        'CV RMSE': r['CV RMSE'], \n",
    "        'Std': r['Std']\n",
    "    })\n",
    "\n",
    "# Voting\n",
    "for r in voting_results:\n",
    "    all_results.append({\n",
    "        'Method': r['Ensemble'], \n",
    "        'Type': 'Voting', \n",
    "        'CV RMSE': r['CV RMSE'], \n",
    "        'Std': r['Std']\n",
    "    })\n",
    "\n",
    "# Stacking\n",
    "for r in stacking_results:\n",
    "    all_results.append({\n",
    "        'Method': r['Ensemble'], \n",
    "        'Type': 'Stacking', \n",
    "        'CV RMSE': r['CV RMSE'], \n",
    "        'Std': r['Std']\n",
    "    })\n",
    "\n",
    "# Blending\n",
    "for r in blend_results:\n",
    "    all_results.append({\n",
    "        'Method': r['Ensemble'], \n",
    "        'Type': 'Blending', \n",
    "        'CV RMSE': r['CV RMSE'], \n",
    "        'Std': r['Std']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(all_results).sort_values('CV RMSE')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ALL METHODS COMPARISON (Top 15, sorted by CV RMSE)\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Plot 1: Top 12 methods\n",
    "ax1 = axes[0]\n",
    "top_12 = results_df.head(12)\n",
    "colors = {\n",
    "    'Single Model': 'steelblue', \n",
    "    'Voting': 'coral', \n",
    "    'Stacking': 'green', \n",
    "    'Blending': 'purple'\n",
    "}\n",
    "bar_colors = [colors[t] for t in top_12['Type']]\n",
    "\n",
    "y_pos = np.arange(len(top_12))\n",
    "bars = ax1.barh(y_pos, top_12['CV RMSE'], xerr=top_12['Std'], \n",
    "                color=bar_colors, edgecolor='black', capsize=3, alpha=0.8)\n",
    "\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(top_12['Method'], fontsize=9)\n",
    "ax1.set_xlabel('CV RMSE (lower is better)', fontsize=11)\n",
    "ax1.set_title('Top 12 Methods by CV RMSE', fontsize=12, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Add value labels\n",
    "for i, val in enumerate(top_12['CV RMSE']):\n",
    "    ax1.text(val + 0.08, i, f'{val:.3f}', va='center', fontsize=9)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=colors[k], label=k) for k in colors]\n",
    "ax1.legend(handles=legend_elements, loc='lower right', fontsize=9)\n",
    "\n",
    "# Plot 2: Average by method type\n",
    "ax2 = axes[1]\n",
    "type_avg = results_df.groupby('Type')['CV RMSE'].agg(['mean', 'std', 'min']).reset_index()\n",
    "type_avg = type_avg.sort_values('mean')\n",
    "\n",
    "bars2 = ax2.bar(type_avg['Type'], type_avg['mean'], yerr=type_avg['std'],\n",
    "                color=[colors[t] for t in type_avg['Type']], \n",
    "                edgecolor='black', capsize=5, alpha=0.8)\n",
    "\n",
    "# Add best in each category\n",
    "for i, (bar, best) in enumerate(zip(bars2, type_avg['min'])):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.15, \n",
    "             f'avg: {bar.get_height():.2f}\\nbest: {best:.2f}', \n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax2.set_ylabel('Average CV RMSE', fontsize=11)\n",
    "ax2.set_title('Performance by Method Type', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylim(10, 12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Generate Submission Files\n",
    "\n",
    "Let's create submission files for the most promising methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final models on full training data\n",
    "print(\"Training final models on full data...\")\n",
    "\n",
    "final_models = {}\n",
    "\n",
    "# Linear models (scaled)\n",
    "final_models['Ridge'] = Ridge(alpha=1.0)\n",
    "final_models['Ridge'].fit(X_scaled.values, y.values)\n",
    "\n",
    "final_models['Lasso'] = Lasso(alpha=0.1)\n",
    "final_models['Lasso'].fit(X_scaled.values, y.values)\n",
    "\n",
    "final_models['ElasticNet'] = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "final_models['ElasticNet'].fit(X_scaled.values, y.values)\n",
    "\n",
    "# Tree models (raw)\n",
    "final_models['RandomForest'] = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "final_models['RandomForest'].fit(X.values, y.values)\n",
    "\n",
    "final_models['GradientBoosting'] = GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "final_models['GradientBoosting'].fit(X.values, y.values)\n",
    "\n",
    "# Get test predictions from each model\n",
    "test_preds = {}\n",
    "test_preds['Ridge'] = final_models['Ridge'].predict(X_test_scaled.values)\n",
    "test_preds['Lasso'] = final_models['Lasso'].predict(X_test_scaled.values)\n",
    "test_preds['ElasticNet'] = final_models['ElasticNet'].predict(X_test_scaled.values)\n",
    "test_preds['RandomForest'] = final_models['RandomForest'].predict(X_test.values)\n",
    "test_preds['GradientBoosting'] = final_models['GradientBoosting'].predict(X_test.values)\n",
    "\n",
    "print(\"All models trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission files\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATING SUBMISSION FILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "submissions = {}\n",
    "\n",
    "# 1. Single best model (ElasticNet)\n",
    "submissions['elasticnet_single'] = test_preds['ElasticNet']\n",
    "\n",
    "# 2. Linear blend (Ridge + Lasso + ElasticNet)\n",
    "submissions['blend_linear'] = (0.33 * test_preds['Ridge'] + \n",
    "                                0.33 * test_preds['Lasso'] + \n",
    "                                0.34 * test_preds['ElasticNet'])\n",
    "\n",
    "# 3. Heavy ElasticNet blend\n",
    "submissions['blend_heavy_en'] = (0.15 * test_preds['Ridge'] + \n",
    "                                  0.15 * test_preds['Lasso'] + \n",
    "                                  0.5 * test_preds['ElasticNet'] + \n",
    "                                  0.1 * test_preds['RandomForest'] + \n",
    "                                  0.1 * test_preds['GradientBoosting'])\n",
    "\n",
    "# 4. ElasticNet + RF (60/40)\n",
    "submissions['blend_en_rf_60_40'] = (0.6 * test_preds['ElasticNet'] + \n",
    "                                     0.4 * test_preds['RandomForest'])\n",
    "\n",
    "# 5. ElasticNet + RF (70/30)\n",
    "submissions['blend_en_rf_70_30'] = (0.7 * test_preds['ElasticNet'] + \n",
    "                                     0.3 * test_preds['RandomForest'])\n",
    "\n",
    "# 6. All 5 equal\n",
    "submissions['blend_all5_equal'] = (test_preds['Ridge'] + \n",
    "                                    test_preds['Lasso'] + \n",
    "                                    test_preds['ElasticNet'] + \n",
    "                                    test_preds['RandomForest'] + \n",
    "                                    test_preds['GradientBoosting']) / 5\n",
    "\n",
    "# Save submission files\n",
    "for name, preds in submissions.items():\n",
    "    submission = pd.DataFrame({'Id': test_fe['Id'], 'pop': preds})\n",
    "    filename = f'./submission_{name}.csv'\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "print(\"\\nAll submissions saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, preds) in enumerate(submissions.items()):\n",
    "    ax = axes[i]\n",
    "    ax.hist(preds, bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(preds.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {preds.mean():.1f}')\n",
    "    ax.set_xlabel('Predicted Popularity')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(name.replace('_', ' ').title(), fontsize=10, fontweight='bold')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Prediction Distributions by Submission', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ENSEMBLE METHODS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. BEST METHODS BY CV RMSE:\")\n",
    "print(\"-\" * 40)\n",
    "print(results_df.head(5).to_string(index=False))\n",
    "\n",
    "print(\"\\n2. KEY FINDINGS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"- Single ElasticNet remains competitive with ensembles\")\n",
    "print(\"- Blending with ElasticNet + small RF contribution works well\")\n",
    "print(\"- Stacking tends to overfit on this small dataset\")\n",
    "print(\"- Linear models consistently outperform tree-based models\")\n",
    "\n",
    "print(\"\\n3. RECOMMENDATION FOR KAGGLE:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Try these submissions (in order of priority):\")\n",
    "print(\"  1. submission_blend_en_rf_60_40.csv (combines linear + non-linear)\")\n",
    "print(\"  2. submission_blend_heavy_en.csv (diversified blend)\")\n",
    "print(\"  3. submission_elasticnet_single.csv (best single model)\")\n",
    "\n",
    "print(\"\\n4. WHY CV != KAGGLE SCORE:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"- Test set may have different distribution\")\n",
    "print(\"- Small dataset = high variance in estimates\")\n",
    "print(\"- Different models may generalize differently\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Ensemble methods combine multiple models** to potentially capture different patterns.\n",
    "\n",
    "2. **Three main approaches:**\n",
    "   - **Voting:** Simple average (equal weights)\n",
    "   - **Stacking:** Meta-learner combines predictions\n",
    "   - **Blending:** Custom weighted average\n",
    "\n",
    "3. **On this dataset:**\n",
    "   - Linear models dominate (ElasticNet, Ridge, Lasso)\n",
    "   - Ensembles don't dramatically improve CV scores\n",
    "   - But blending linear + tree models may generalize better\n",
    "\n",
    "4. **Practical advice:**\n",
    "   - CV performance is a guide, not a guarantee\n",
    "   - Try multiple submissions to find what works on the test set\n",
    "   - Diversity in model types can help generalization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
